---
title: Empirical Dynamic Modelling
subtitle: Automatic Causal Inference and Forecasting<br>TSF 2022
author: Dr Patrick Laub
date: 2 December 2022
date-format: long
format:
  revealjs:
    theme: [serif, custom.scss]
    controls: true
    controls-tutorial: true
    logo: unsw-logo.svg
    title-slide-attributes:
      data-background-image: unsw-yellow-shape.png
      data-background-size: contain !important
    slide-number: c/t
    strip-comments: true
    margin: 0.2
    chalkboard:
      boardmarker-width: 5
      grid: false
    include-before: <div class="line right"></div>
    footer: Patrick Laub, Time-Series and Forecasting Symposium, University of Sydney
highlight-style: breeze
execute:
  echo: true
---

<!--
## Plan

1. ðŸª„A magic trickðŸª„ (Taken's theorem)
2. Stata package
3. Automated analysis
-->

# Introduction {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## About me

- Software engineer & mathematics (UQ)
- PhD in computational applied probability (Aarhus)
- Actuarial science post-doc (Lyon)
- Research software engineer (Uni Melbourne)
- Actuarial science lecturer (UNSW)

Cf. [https://pat-laub.github.io](https://pat-laub.github.io).

## Goal: automatic causal inference

<br>

``` r
df <- read.csv("chicago.csv")
head(df)
#>   Time Temperature Crime
#> 1    1       24.08  1605
#> 2    2       19.04  1119
#> 3    3       28.04  1127
#> 4    4       30.02  1154
#> 5    5       35.96  1251
#> 6    6       33.08  1276

library(fastEDM)

crimeCCMCausesTemp <- easy_edm("Crime", "Temperature", data=df)
#> âœ– No evidence of CCM causation from Crime to Temperature found.

tempCCMCausesCrime <- easy_edm("Temperature", "Crime", data=df)
#> âœ” Some evidence of CCM causation from Temperature to Crime found.
```

## {.smaller}

<div style="border: 3px dashed #555"><img src="stata-journal-paper-title.png" /></div>

::: columns
::: column
Jinjing Li<br>
University of Canberra

George Sugihara<br>
University of California San Diego
:::
::: column
Michael J. Zyphur<br>
University of Queensland

Patrick J. Laub<br>
UNSW
:::
:::

<br>

::: callout-note
##  Acknowledgments

Discovery Project DP200100219 and Future Fellowship FT140100629.
:::

## A different view of causality

<br>

Imagine $x_t$, $y_t$, $z_t$ are interesting time series...

_If_ the data is generated according to the nonlinear system:

$$
\begin{aligned}
  x_{t+1}  &= \sigma (y_t - x_t) \\
  y_{t+1}  &= x_t (\rho - z_t) - y_t \\
  z_{t+1}  &= x_t y_t - \beta z_t
\end{aligned}
$$

then we can say $y \Rightarrow x$, both $x, z \Rightarrow y$, and both $x, y \Rightarrow z$.

## Linear/nonlinear dynamical systems

<br>

Say $\mathbf{x}_t = (x_t, y_t, z_t)$, then if:

::: columns
::: column

$$ \mathbf{x}_{t+1} = \mathbf{A} \mathbf{x}_{t} $$

we have a linear system.

:::
::: column

$$ \mathbf{x}_{t+1} = f(\mathbf{x}_{t}) $$

we have a nonlinear system.

:::
:::

> Using a term like nonlinear science is like referring to the bulk of zoology as the study of non-elephant animals. (StanisÅ‚aw Ulam)

::: footer
We don't fit a model for $f$, non-parametrically use the data.
Hence the name _empirical_ dynamic modelling. 
:::

## Noise / unobserved variables?

Takens' theorem to the rescue, though...

> Takens' theorem is a deep mathematical result with far-reaching implications.
> Unfortunately, to really understand it, it requires a background in topology.
> (Munch et al. 2020)

<iframe src="https://en.wikipedia.org/wiki/Takens%27s_theorem#Simplified,_slightly_inaccurate_version" width="100%" height="250" style="border:none;"></iframe>

::: footer
Source: Munch et al. (2020), Frequently asked questions about nonlinear dynamics and empirical dynamic modelling, ICES Journal of Marine Science.
:::

# Empirical Dynamic Modelling (EDM) {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Create lagged embeddings

<br>
<br>

Given $\{ x_t \}_{t\in\mathbb{N}}$ time series, create $E$-length trajectories

$$ \overleftarrow{\mathbf{x}}_t = (x_t, x_{t-1}, x_{t-2}, \dots, x_{t-(E-1)}) \in \mathbb{R}^{E} $$

and targets

$$ y_t = (x_{t+1}) .$$

::: {.callout-note}
The $\overleftarrow{\mathbf{x}}_t$'s are called _points_ (on the shadow manifold).
:::

<!-- 
## Key idea {data-visibility="uncounted"}

Use lags of the time series!

Given $x_t$ time series, create $E$-length trajectories

$$ \overleftarrow{\mathbf{x}}_t = (x_t, x_{t-\tau}, x_{t-2\tau}, \dots, x_{t-(E-1)\tau}) \in \mathbb{R}^{E} , t=1,2,\dots$$

and targets

$$ y_t = (x_{t+p}) , t=1,2,\dots .$$

Parametrised by $\tau, p \in \mathbb{N}$.

-->

## Split the data

Say $\mathcal{L} = \{ \overleftarrow{\mathbf{x}}_1 , \overleftarrow{\mathbf{x}}_2 , \dots , \overleftarrow{\mathbf{x}}_{T/2} \}$
is the _library_ ($\approx$ train) set.

And $\mathcal{P} = \{ \overleftarrow{\mathbf{x}}_{T/2+1} , \dots , \overleftarrow{\mathbf{x}}_{T} \}$
is the _prediction_ ($\approx$ test) set.

<br>

For point $\overleftarrow{\mathbf{x}}_{s} \in \mathcal{P}$, find $k$ nearest neighbours in $\mathcal{L}$.
Say, e.g., $k=2$ and the neighbours are 

$$\mathcal{NN}_k = \bigl( (\overleftarrow{\mathbf{x}}_{3}, y_3), (\overleftarrow{\mathbf{x}}_{5}, y_5) \bigr)$$

::: {.callout-note}
Calculating the distances from $\overleftarrow{\mathbf{x}}_{s} \in \mathcal{P}$ to every point in $\mathcal{L}$ and finding nearest neighbours can be computationally demanding.
:::

## Non-parametric prediction

::: columns
::: column

Given $\overleftarrow{\mathbf{x}}_{s} \in \mathcal{P}$, has the neighbours

:::
::: column

$$\mathcal{NN}_k = \bigl( (\overleftarrow{\mathbf{x}}_{3}, y_3), (\overleftarrow{\mathbf{x}}_{5}, y_5) \bigr)$$

:::
:::

::: columns
::: column

**Simplex method**:

Predict

::: 
::: column

$$\widehat{y}_s = w_1 y_3 + w_2 y_5 .$$ 

::: 
::: 

<br>

::: columns
::: column

**S-map method**:

Build a local linear system
and predict using a local autoregressive style.

::: 
::: column

$$\widehat{y}_s = \mathbf{A}_s \overleftarrow{\mathbf{x}}_s$$

::: 
::: 

## Causation

<br>

If $x_t$ causes $y_t$, then information about $x_t$ is somehow embedded in $y_t$.

By observing $y_t$, we should be able to (roughly) forecast $x_t$.

By observing more of $y_t$ (more "training data"), our forecasts of $x_t$ should be more accurate.

This is the core idea behind __convergent cross mapping__.

<!--
## Fish brains

<iframe width="100%" height="600" src="https://www.youtube.com/embed/5HtXYKKRA8g?start=1311" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
-->

# Software {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Stata package

<iframe src="https://edm-developers.github.io/edm-stata/" width="100%" height="600" style="border:none;"></iframe>

## R package

<iframe src="https://edm-developers.github.io/fastEDM-r/" width="100%" height="600" style="border:none;"></iframe>

## Python package

<iframe src="https://edm-developers.github.io/fastEDM-python/" width="100%" height="400" style="border:none;"></iframe>

::: {.callout-note}
Thanks to Rishi Dhushiyandan for his hard work on `easy_edm`.
:::

<!--
## Sydney house price forecasting

```{python}
import fastEDM
dir(fastEDM)
```
-->

<!--
## Package features

- Fast, optimised multithreaded C++ code
- Easy to use automated analysis
- Handles panel data
- Handles irregularly sampled time series / missing data
- Highly "vectorised" (7 for loops deep)
- GPU acceleration

- Well-engineered (3 OS, CI, count # LOC tessts)
-->